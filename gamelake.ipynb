{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                \n",
      "Q-table:\n",
      "\n",
      "       left     right\n",
      "0  0.000000  0.004320\n",
      "1  0.000000  0.025005\n",
      "2  0.000030  0.111241\n",
      "3  0.000000  0.368750\n",
      "4  0.027621  0.745813\n",
      "5  0.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "np.random.seed(2)  # reproducible\n",
    "\n",
    "\n",
    "N_STATES = 6   # the length of the 1 dimensional world\n",
    "ACTIONS = ['left', 'right']     # available actions\n",
    "EPSILON = 0.9   # greedy police\n",
    "ALPHA = 0.1     # learning rate\n",
    "GAMMA = 0.9    # discount factor\n",
    "MAX_EPISODES = 13   # maximum episodes\n",
    "FRESH_TIME = 0.3    # fresh time for one move\n",
    "\n",
    "\n",
    "def build_q_table(n_states, actions):\n",
    "    table = pd.DataFrame(\n",
    "        np.zeros((n_states, len(actions))),     # q_table initial values\n",
    "        columns=actions,    # actions's name\n",
    "    )\n",
    "    # print(table)    # show table\n",
    "    return table\n",
    "\n",
    "\n",
    "def choose_action(state, q_table):\n",
    "    # This is how to choose an action\n",
    "    state_actions = q_table.iloc[state, :]\n",
    "    if (np.random.uniform() > EPSILON) or ((state_actions == 0).all()):  # act non-greedy or state-action have no value\n",
    "        action_name = np.random.choice(ACTIONS)\n",
    "    else:   # act greedy\n",
    "        action_name = state_actions.idxmax()    # replace argmax to idxmax as argmax means a different function in newer version of pandas\n",
    "    return action_name\n",
    "\n",
    "\n",
    "def get_env_feedback(S, A):\n",
    "    # This is how agent will interact with the environment\n",
    "    if A == 'right':    # move right\n",
    "        if S == N_STATES - 2:   # terminate\n",
    "            S_ = 'terminal'\n",
    "            R = 1\n",
    "        else:\n",
    "            S_ = S + 1\n",
    "            R = 0\n",
    "    else:   # move left\n",
    "        R = 0\n",
    "        if S == 0:\n",
    "            S_ = S  # reach the wall\n",
    "        else:\n",
    "            S_ = S - 1\n",
    "    return S_, R\n",
    "\n",
    "\n",
    "def update_env(S, episode, step_counter):\n",
    "    # This is how environment be updated\n",
    "    env_list = ['-']*(N_STATES-1) + ['T']   # '---------T' our environment\n",
    "    if S == 'terminal':\n",
    "        interaction = 'Episode %s: total_steps = %s' % (episode+1, step_counter)\n",
    "        print('\\r{}'.format(interaction), end='')\n",
    "        time.sleep(2)\n",
    "        print('\\r                                ', end='')\n",
    "    else:\n",
    "        env_list[S] = 'o'\n",
    "        interaction = ''.join(env_list)\n",
    "        print('\\r{}'.format(interaction), end='')\n",
    "        time.sleep(FRESH_TIME)\n",
    "\n",
    "\n",
    "def rl():\n",
    "    # main part of RL loop\n",
    "    q_table = build_q_table(N_STATES, ACTIONS)\n",
    "    for episode in range(MAX_EPISODES):\n",
    "        step_counter = 0\n",
    "        S = 0\n",
    "        is_terminated = False\n",
    "        update_env(S, episode, step_counter)\n",
    "        while not is_terminated:\n",
    "\n",
    "            A = choose_action(S, q_table)\n",
    "            S_, R = get_env_feedback(S, A)  # take action & get next state and reward\n",
    "            q_predict = q_table.loc[S, A]\n",
    "            if S_ != 'terminal':\n",
    "                q_target = R + GAMMA * q_table.iloc[S_, :].max()   # next state is not terminal\n",
    "            else:\n",
    "                q_target = R     # next state is terminal\n",
    "                is_terminated = True    # terminate this episode\n",
    "\n",
    "            q_table.loc[S, A] += ALPHA * (q_target - q_predict)  # update\n",
    "            S = S_  # move to next state\n",
    "\n",
    "            update_env(S, episode, step_counter+1)\n",
    "            step_counter += 1\n",
    "    return q_table\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    q_table = rl()\n",
    "    print('\\r\\nQ-table:\\n')\n",
    "    print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                \n",
      "Q-table:\n",
      "\n",
      "       left         right\n",
      "0  0.000000  0.000000e+00\n",
      "1  0.651322  2.268000e-03\n",
      "2  0.244485  2.217284e-03\n",
      "3  0.067385  0.000000e+00\n",
      "4  0.012487  5.904900e-07\n",
      "5  0.001490  0.000000e+00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "np.random.seed(2)  # reproducible\n",
    "\n",
    "\n",
    "N_STATES = 6   # the length of the 1 dimensional world\n",
    "ACTIONS = ['left', 'right']     # available actions\n",
    "EPSILON = 0.9   # greedy police\n",
    "ALPHA = 0.1     # learning rate\n",
    "GAMMA = 0.9    # discount factor\n",
    "MAX_EPISODES = 10   # maximum episodes\n",
    "FRESH_TIME = 0.3    # fresh time for one move\n",
    "\n",
    "\n",
    "def build_q_table(n_states, actions):\n",
    "    table = pd.DataFrame(\n",
    "        np.zeros((n_states, len(actions))),     # q_table initial values\n",
    "        columns=actions,    # actions's name\n",
    "    )\n",
    "    # print(table)    # show table\n",
    "    return table\n",
    "\n",
    "\n",
    "def choose_action(state, q_table):\n",
    "    # This is how to choose an action\n",
    "    state_actions = q_table.iloc[state, :]\n",
    "    if (np.random.uniform() > EPSILON) or ((state_actions == 0).all()):  # act non-greedy or state-action have no value\n",
    "        action_name = np.random.choice(ACTIONS)\n",
    "    else:   # act greedy\n",
    "        action_name = state_actions.idxmax()    # replace argmax to idxmax as argmax means a different function in newer version of pandas\n",
    "    return action_name\n",
    "\n",
    "\n",
    "def get_env_feedback(S, A):\n",
    "    # This is how agent will interact with the environment\n",
    "    if A == 'left':    # move LEFT\n",
    "        if S == 1:   # terminate\n",
    "            S_ = 'terminal'\n",
    "            R = 1\n",
    "        else:\n",
    "            S_ = S - 1\n",
    "            R = 0\n",
    "    else:   # move right\n",
    "        R = 0\n",
    "        if S == N_STATES-1:\n",
    "            S_ = N_STATES-1  # reach the wall\n",
    "        else:\n",
    "            S_ = S + 1\n",
    "    return S_, R\n",
    "\n",
    "\n",
    "def update_env(S, episode, step_counter):\n",
    "    # This is how environment be updated\n",
    "    env_list =['T']+['_']*(N_STATES-1)   # 'T---------' our environment\n",
    "    if S == 'terminal':\n",
    "        interaction = 'Episode %s: total_steps = %s' % (episode+1, step_counter)\n",
    "        print('\\r{}'.format(interaction), end='')\n",
    "        time.sleep(2)\n",
    "        print('\\r                                ', end='')\n",
    "    else:\n",
    "        env_list[S] = 'O'\n",
    "        interaction = ''.join(env_list)\n",
    "        print('\\r{}'.format(interaction), end='')\n",
    "        time.sleep(FRESH_TIME)\n",
    "\n",
    "\n",
    "def rl():\n",
    "    # main part of RL loop\n",
    "    q_table = build_q_table(N_STATES, ACTIONS)\n",
    "    for episode in range(MAX_EPISODES):\n",
    "        step_counter = 0\n",
    "        S = N_STATES-1\n",
    "        is_terminated = False\n",
    "        update_env(S, episode, step_counter)\n",
    "        while not is_terminated:\n",
    "\n",
    "            A = choose_action(S, q_table)\n",
    "            S_, R = get_env_feedback(S, A)  # take action & get next state and reward\n",
    "            q_predict = q_table.loc[S, A]\n",
    "            if S_ != 'terminal':\n",
    "                q_target = R + GAMMA * q_table.iloc[S_, :].max()   # next state is not terminal\n",
    "            else:\n",
    "                q_target = R     # next state is terminal\n",
    "                is_terminated = True    # terminate this episode\n",
    "\n",
    "            q_table.loc[S, A] += ALPHA * (q_target - q_predict)  # update\n",
    "            S = S_  # move to next state\n",
    "\n",
    "            update_env(S, episode, step_counter+1)\n",
    "            step_counter += 1\n",
    "    return q_table\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    q_table = rl()\n",
    "    print('\\r\\nQ-table:\\n')\n",
    "    print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-24-5695e5fe7633>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-24-5695e5fe7633>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    rom tetrisrl.environment import Environment,Action\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "rom tetrisrl.environment import Environment,Action\n",
    "from tetrisrl.baseline import LowestCenterOfGravityAgent\n",
    "from tetrisrl.agents import HumanAgent, RandomAgent\n",
    "from tetrisrl.rl import QLearningAgent\n",
    "from tetrisrl.serialize import ObservationSerializer\n",
    "import json\n",
    "import random\n",
    "import sys\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "\n",
    "class Globals(object):\n",
    "    SCREEN_DIMS=(640,480)\n",
    "\n",
    "class Colors(object):\n",
    "    WHITE=(255,255,255)\n",
    "    GRAY=(128,128,128)\n",
    "    GREEN=(0,255,0)\n",
    "    RED=(255,0,0)\n",
    "    BLUE=(0,0,255)\n",
    "    BLACK=(0,0,0)\n",
    "\n",
    "class Engine(object):\n",
    "    def __init__(self, environment,agent,config,output_dir):\n",
    "        self.environment=environment\n",
    "        self.s = self.environment.initial_state()\n",
    "        self.total_pos_r = 0.0\n",
    "        self.total_neg_r = 0.0\n",
    "        self.agent=agent\n",
    "        self.fps = config[\"fps\"]\n",
    "        self.show = config[\"show\"]\n",
    "        self.max_time = config[\"max_time\"]\n",
    "        self.output_dir = output_dir\n",
    "        self.oserializer = ObservationSerializer()\n",
    "        self.debug_mode = config[\"debug_mode\"]\n",
    "\n",
    "        if config[\"log_observations\"]:\n",
    "            self.obs_log_file = open(\"{}/observations.json\".format(output_dir),\"w\")\n",
    "        else:\n",
    "            self.obs_log_file = None\n",
    "\n",
    "        if config[\"replay_observations\"]:\n",
    "            for i in range(config[\"replay_count\"]):\n",
    "                for fn in config[\"replay_observations\"]:\n",
    "                    self.replay_observations(fn)\n",
    "\n",
    "        if self.show:\n",
    "            import pygame\n",
    "            from pygame.locals import *\n",
    "            pygame.init()\n",
    "            self.font = pygame.font.SysFont(None, 28)    \n",
    "            self.screen=pygame.display.set_mode(Globals.SCREEN_DIMS,0,32)\n",
    "            self.clock = pygame.time.Clock()\n",
    "            pygame.display.set_caption(\"Tetris\")\n",
    "            self.draw()\n",
    "\n",
    "    def replay_observations(self, fn):\n",
    "        with open(fn) as fin:\n",
    "            print \"Replaying from file: {}...\".format(fn)\n",
    "            for l in fin:\n",
    "                s,a,r,sprime,pfbm = self.oserializer.deserialize_json(json.loads(l))\n",
    "                self.agent.observe_sars_tuple(s,a,r,sprime,pfbm=pfbm)\n",
    "\n",
    "    def detect_quit(self):\n",
    "        if self.show:\n",
    "            if pygame.event.peek(QUIT):\n",
    "                pygame.quit()\n",
    "                sys.exit()\n",
    "\n",
    "    def loop(self):\n",
    "        def bitmap_mean_active_column(b):\n",
    "            if b.sum()==0:\n",
    "                return 0\n",
    "            else:\n",
    "                cols = b.nonzero()[1]\n",
    "                return cols.mean()\n",
    "        self.draw()\n",
    "        t = 0\n",
    "        start = time.clock()\n",
    "        while True:\n",
    "            t += 1\n",
    "            if t % 1000 == 0:\n",
    "                self.agent.save_model(\"{}/model.{:06d}iters\".format(self.output_dir, t))\n",
    "\n",
    "            if t > self.max_time:\n",
    "                break\n",
    "\n",
    "            if self.show:\n",
    "                self.clock.tick(self.fps)\n",
    "            self.detect_quit()\n",
    "            a,debug_info = self.agent.act(self.s,debug_mode=self.debug_mode)\n",
    "            if self.debug_mode:\n",
    "                for pfbm in sorted(debug_info[\"pfbms\"], cmp=lambda x,y: cmp(bitmap_mean_active_column(x),bitmap_mean_active_column(y))):\n",
    "                    self.clock.tick(6)\n",
    "                    self.draw_bitmap(pfbm)\n",
    "                    pygame.display.update()\n",
    "                    self.clock.tick(6)\n",
    "            sprime,r,pfbm,rcounts = self.environment.next_state_and_reward(self.s, a)\n",
    "            if \"rows_cleared\" in rcounts:\n",
    "                logging.info(\"ROWS_CLEARED: {}\".format(rcounts[\"rows_cleared\"]))\n",
    "            if \"game_over\" in rcounts:\n",
    "                logging.info(\"GAME_OVER\")\n",
    "\n",
    "            if r > 0:\n",
    "                self.total_pos_r += r\n",
    "            else:\n",
    "                self.total_neg_r += r\n",
    "\n",
    "            if self.obs_log_file:\n",
    "                self.obs_log_file.write(\"{}\\n\".format(json.dumps(self.oserializer.serialize_json(self.s,a,r,sprime,pfbm=pfbm))))\n",
    "\n",
    "            self.agent.observe_sars_tuple(self.s,a,r,sprime,pfbm=pfbm)\n",
    "            self.s = sprime\n",
    "            self.draw()\n",
    "            duration = time.clock()-start\n",
    "            print \"Runtime={:.2f}s  T={}  Total Reward: {:.2f}  {:.2f}\".format(duration, t, self.total_pos_r, self.total_neg_r)\n",
    "            \n",
    "    def draw(self):\n",
    "        if not self.show:\n",
    "            return\n",
    "\n",
    "        self.screen.fill(Colors.BLACK)\n",
    "        w = 20\n",
    "        b = self.s.arena.bitmap\n",
    "        ls = self.s.lshape\n",
    "\n",
    "        text = self.font.render(\"Total Reward: {:.2f}  {:.2f}\".format(self.total_pos_r, self.total_neg_r), True, Colors.WHITE, Colors.BLUE)\n",
    "        textRect = text.get_rect()\n",
    "        textRect.centerx = (w * b.shape[1]) + 250\n",
    "        textRect.centery = self.screen.get_rect().centery\n",
    "        self.screen.blit(text, textRect)\n",
    "        \n",
    "        self.draw_bitmap(b)\n",
    "        self.draw_lshape(ls)\n",
    "\n",
    "        pygame.display.update()\n",
    "\n",
    "    def draw_bitmap(self, b):\n",
    "        w = 20\n",
    "        for r in range(b.shape[0]):\n",
    "            for c in range(b.shape[1]):\n",
    "                rect = (w+(w*c),w+(w*r),w,w)\n",
    "                if b[r,c]:\n",
    "                    pygame.draw.rect(self.screen, Colors.GREEN, rect)\n",
    "                else:\n",
    "                    pygame.draw.rect(self.screen, Colors.GRAY, rect)\n",
    "\n",
    "    def draw_lshape(self, ls):\n",
    "        w = 20\n",
    "        for coord in ls.coords():\n",
    "            r,c = (coord[0],coord[1])\n",
    "            pygame.draw.rect(self.screen, Colors.BLUE, (w+(w*c),w+(w*r),w,w))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Modifies in place\n",
    "def override_config(c, o):\n",
    "    for key,val in o.iteritems():\n",
    "        keys = key.split(\"/\")\n",
    "        p = c\n",
    "        for k in keys[:-1]:\n",
    "            p = p[k]\n",
    "        p[keys[-1]] = val\n",
    "\n",
    "def process_config(cf, of, sf):\n",
    "    with open(cf,\"r\") as fin:\n",
    "        config = json.load(fin)\n",
    "\n",
    "    if of.lower() != \"none\": \n",
    "        with open(of,\"r\") as fin:\n",
    "            override = json.load(fin)\n",
    "        override_config(config, override)\n",
    "        \n",
    "    with open(sf, \"w\") as fout:\n",
    "        json.dump(config, fout, indent=4, sort_keys=True)\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "\n",
    "config_file = sys.argv[1]\n",
    "config_override_file = sys.argv[2]\n",
    "output_dir = sys.argv[3]\n",
    "\n",
    "# output dir\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# config\n",
    "saved_config_file = \"{}/config\".format(output_dir)\n",
    "config = process_config(config_file, config_override_file, saved_config_file)\n",
    "\n",
    "# log file\n",
    "log_file = \"{}/log\".format(output_dir)\n",
    "logging.basicConfig(filename=log_file, filemode=\"w\", level=logging.DEBUG)\n",
    "\n",
    "\n",
    "# initialize everything\n",
    "e = Environment(config[\"environment\"])\n",
    "\n",
    "agent_type = config[\"agent\"][\"type\"]\n",
    "\n",
    "if agent_type == \"rl\":\n",
    "    agent = QLearningAgent(e,config[\"agent\"])\n",
    "elif agent_type == \"lcog\":\n",
    "    agent = LowestCenterOfGravityAgent(e)\n",
    "elif agent_type == \"human\":\n",
    "    agent = HumanAgent()\n",
    "    assert config[\"engine\"][\"show\"]\n",
    "elif agent_type == \"random\":\n",
    "    agent = RandomAgent()\n",
    "else:\n",
    "    raise Exception(\"Unknown agent type: {}\".format(config[\"agent\"][\"type\"]))\n",
    "\n",
    "engine = Engine(e,agent,config[\"engine\"],output_dir)\n",
    "engine.loop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-23-a2462e0d3442>, line 60)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-23-a2462e0d3442>\"\u001b[1;36m, line \u001b[1;32m60\u001b[0m\n\u001b[1;33m    print \"name: {},   stat_name={}\".format(name,stat_name)\u001b[0m\n\u001b[1;37m                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def windowed_average(a, wsize):\n",
    "    results = []\n",
    "    if not a:\n",
    "        return [],[]\n",
    "    s = sum(a[:wsize])\n",
    "    xs = []\n",
    "    for i in range(wsize-1,len(a)):\n",
    "        xs.append(i)\n",
    "        results.append(s / wsize)\n",
    "        s -= a[i+1-wsize]\n",
    "        if i < len(a)-1:\n",
    "            s += a[i+1]\n",
    "    return xs,results\n",
    "\n",
    "def read_windowed_stats(fn, windows):\n",
    "    r = []\n",
    "    d = []\n",
    "    absd = []\n",
    "\n",
    "    with open(fn) as fin:\n",
    "        for line in fin:\n",
    "            tokens = line.strip().split()\n",
    "            if line.startswith(\"INFO:root:DELTA:\"):\n",
    "                d.append(float(tokens[-1]))\n",
    "                absd.append(abs(float(tokens[-1])))\n",
    "            if line.startswith(\"INFO:root:REWARD:\"):\n",
    "                r.append(float(tokens[-1]))\n",
    "    return {\n",
    "        \"reward\": windowed_average(r, windows[\"reward\"]), \n",
    "        \"delta\": windowed_average(d, windows[\"delta\"]),\n",
    "        \"abs_delta\": windowed_average(absd, windows[\"abs_delta\"])\n",
    "        }\n",
    "\n",
    "def make_plot(fn, xlabel, ylabel, title, stats, stat_name):\n",
    "    #line_styles = [\"r--\", \"b-.\", \"g:\", \"o-\", \"p:\"]\n",
    "    line_styles = [\n",
    "        {\n",
    "            \"color\": \"blue\"\n",
    "        },\n",
    "        {\n",
    "            \"color\": \"green\"\n",
    "        },\n",
    "        {\n",
    "            \"color\": \"orange\"\n",
    "        },\n",
    "        {\n",
    "            \"color\": \"red\"\n",
    "        },\n",
    "        {\n",
    "            \"color\": \"pink\"\n",
    "        },\n",
    "    ]\n",
    "    for i,name in enumerate(stats.keys()):\n",
    "        print \"name: {},   stat_name={}\".format(name,stat_name)\n",
    "        xs,ys = stats[name][stat_name]\n",
    "        kwargs = line_styles[i]\n",
    "        plt.plot(xs,ys,label=name, **kwargs)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xlabel(xlabel) \n",
    "    plt.title(title)\n",
    "    plt.legend(loc=3)\n",
    "    plt.savefig(fn)\n",
    "    plt.clf()\n",
    "    \n",
    "\n",
    "def plot_results(output_dir, input_dirs, windows):\n",
    "\n",
    "    stats = OrderedDict()\n",
    "    for input_dir in input_dirs:\n",
    "        name = input_dir.split(\"/\")[-1]\n",
    "        log = \"{}/log\".format(input_dir)\n",
    "        stats[name] = read_windowed_stats(log, windows)\n",
    "\n",
    "    make_plot(\"{}/delta.png\".format(output_dir), \"Timestep (block placements)\", \"Delta\", \"Trailing average of {} latest delta values\".format(windows[\"delta\"]), stats, \"delta\")\n",
    "    make_plot(\"{}/abs_delta.png\".format(output_dir), \"Timestep (block placements)\", \"Delta magnitude\", \"Trailing average of {} latest delta magnitudes\".format(windows[\"abs_delta\"]), stats, \"abs_delta\")\n",
    "    make_plot(\"{}/reward.png\".format(output_dir), \"Timestep (game ticks)\", \"Reward\", \"Trailing average of {} latest reward values\".format(windows[\"reward\"]), stats, \"reward\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    output_dir = \"plots/{}\".format(sys.argv[1])\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    windows = {\"delta\": 500,\n",
    "               \"abs_delta\": 500,\n",
    "               \"reward\": 30000}\n",
    "\n",
    "    plot_results(output_dir, sys.argv[2:], windows)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
